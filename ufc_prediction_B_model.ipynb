{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0Me30qkKpA+/j10IxSgVl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LastHopeForRaoha/11-22-24/blob/main/ufc_prediction_B_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0VJN4iRJCSc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "import warnings\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "# Suppress warnings and unnecessary logs\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "logging.getLogger(\"lightgbm\").setLevel(logging.CRITICAL)\n",
        "logging.getLogger(\"xgboost\").setLevel(logging.CRITICAL)\n",
        "\n",
        "# File path\n",
        "file_path = \"/content/ULTIMATE_DATABASE.csv\"\n",
        "\n",
        "# Step 1: Load and Filter Dataset\n",
        "def load_and_filter_dataset(file_path):\n",
        "    print(\"\\n--- Step 1: Loading and Filtering Dataset ---\")\n",
        "    dataset = pd.read_csv(file_path, low_memory=False)\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    dataset = dataset.loc[:, ~dataset.columns.duplicated()]\n",
        "\n",
        "    # Filter out women's fights and REDFLAG == False\n",
        "    print(\"Filtering out women's fights and invalid rows...\")\n",
        "    dataset = dataset[~dataset['weight_class'].str.contains(\"Women\", case=False, na=False)]\n",
        "    if 'REDFLAG' in dataset.columns:\n",
        "        dataset = dataset[dataset['REDFLAG'] == True]\n",
        "\n",
        "    # Replace missing values with placeholders\n",
        "    placeholder_map = {\n",
        "        'R_method_of_win': \"Unknown\",\n",
        "        'B_method_of_win': \"Unknown\",\n",
        "        'R_round_of_win': -1,\n",
        "        'B_round_of_win': -1\n",
        "    }\n",
        "    for col, placeholder in placeholder_map.items():\n",
        "        if col in dataset.columns:\n",
        "            dataset[col] = dataset[col].fillna(placeholder)\n",
        "            print(f\"Filled missing values in '{col}' with '{placeholder}'\")\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found!\")\n",
        "\n",
        "    # Drop rows missing 'Winner'\n",
        "    if 'Winner' in dataset.columns:\n",
        "        print(\"Dropping rows missing 'Winner' column...\")\n",
        "        dataset = dataset.dropna(subset=['Winner'])\n",
        "    else:\n",
        "        print(\"Error: 'Winner' column not found!\")\n",
        "        sys.exit()\n",
        "\n",
        "    print(f\"Dataset filtered to {dataset.shape[0]} rows and {dataset.shape[1]} columns.\")\n",
        "    return dataset\n",
        "\n",
        "# Step 2: Preprocess Dataset\n",
        "def preprocess_dataset(dataset, target_column):\n",
        "    print(f\"\\n--- Step 2: Preprocessing Dataset for Target '{target_column}' ---\")\n",
        "    if target_column not in dataset.columns:\n",
        "        print(f\"Target column '{target_column}' not found. Skipping...\")\n",
        "        return None, None\n",
        "\n",
        "    # Handle low-frequency classes\n",
        "    y = dataset[target_column]\n",
        "    class_counts = y.value_counts()\n",
        "    y = y[y.isin(class_counts[class_counts > 1].index)]\n",
        "    dataset = dataset.loc[y.index]\n",
        "\n",
        "    # Drop target column and separate features\n",
        "    X = dataset.drop(columns=[target_column], errors='ignore')\n",
        "    num_cols = X.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
        "    cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "    # Preprocessing pipelines\n",
        "    num_pipeline = Pipeline([('imputer', SimpleImputer(strategy='mean')), ('scaler', StandardScaler())])\n",
        "    cat_pipeline = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(drop='first'))])\n",
        "    preprocessor = ColumnTransformer([('num', num_pipeline, num_cols), ('cat', cat_pipeline, cat_cols)])\n",
        "\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "    print(f\"Processed dataset has {X_preprocessed.shape[1]} features.\")\n",
        "    return X_preprocessed, y\n",
        "\n",
        "# Step 3: Hyperparameter Tuning\n",
        "def hyperparameter_tuning(model, param_grid, X_train, y_train):\n",
        "    print(f\"Performing hyperparameter tuning for {type(model).__name__}...\")\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='accuracy', verbose=0)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(\"Best Parameters:\", grid_search.best_params_)\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# Step 4: Train, Evaluate, and Save Models\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, task_name):\n",
        "    print(f\"\\n--- Training and Evaluating Model for {task_name} ---\")\n",
        "    models = {\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "        'CatBoost': CatBoostClassifier(verbose=0),\n",
        "        'LightGBM': LGBMClassifier(verbose=-1)\n",
        "    }\n",
        "\n",
        "    # Hyperparameter tuning for each model\n",
        "    param_grids = {\n",
        "        'XGBoost': {'n_estimators': [100, 200], 'max_depth': [3, 5]},\n",
        "        'CatBoost': {'depth': [6, 8], 'iterations': [200, 300]},\n",
        "        'LightGBM': {'n_estimators': [100, 200], 'learning_rate': [0.05, 0.1]}\n",
        "    }\n",
        "    tuned_models = {}\n",
        "    for name, model in models.items():\n",
        "        tuned_models[name] = hyperparameter_tuning(model, param_grids[name], X_train, y_train)\n",
        "\n",
        "    # Stacked model with meta-learner\n",
        "    print(\"Building Stacked Ensemble Model...\")\n",
        "    stack = StackingClassifier(\n",
        "        estimators=[(name, model) for name, model in tuned_models.items()],\n",
        "        final_estimator=LogisticRegression(max_iter=1000),\n",
        "        cv=3\n",
        "    )\n",
        "    stack.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = stack.predict(X_test)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "    # Save models\n",
        "    print(\"\\nSaving Models...\")\n",
        "    for name, model in tuned_models.items():\n",
        "        model.save_model(f\"{name}_model.json\")\n",
        "        print(f\"{name} model saved as '{name}_model.json'\")\n",
        "    joblib.dump(stack, \"stacked_model.pkl\")\n",
        "    print(\"Stacked Model saved as 'stacked_model.pkl'\")\n",
        "\n",
        "# Step 5: Execute Task\n",
        "def execute_task(dataset, target_column, task_name):\n",
        "    X, y = preprocess_dataset(dataset, target_column)\n",
        "    if X is None or y is None or y.nunique() < 2:\n",
        "        print(f\"Skipping {task_name}: Not enough data.\")\n",
        "        return\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    train_and_evaluate(X_train, y_train, X_test, y_test, task_name)\n",
        "\n",
        "# Main Execution\n",
        "print(\"Starting UFC Fight Prediction Pipeline...\")\n",
        "dataset = load_and_filter_dataset(file_path)\n",
        "\n",
        "if not dataset.empty:\n",
        "    execute_task(dataset, 'Winner', \"Win/Loss Prediction\")\n",
        "    execute_task(dataset, 'R_method_of_win', \"Method of Victory Prediction\")\n",
        "    execute_task(dataset, 'B_method_of_win', \"Method of Victory Prediction (Blue)\")\n",
        "    execute_task(dataset, 'R_round_of_win', \"Round Prediction\")\n",
        "    execute_task(dataset, 'B_round_of_win', \"Round Prediction (Blue)\")\n",
        "else:\n",
        "    print(\"Dataset is empty after filtering. Please check the input data.\")\n",
        "print(\"\\nPipeline Completed Successfully!\")\n"
      ]
    }
  ]
}